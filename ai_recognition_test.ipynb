{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Enrique\\OneDrive\\Documentos\\ai_voice_recognition\\pyenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "import wave\n",
    "import pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##try - except - finally\n",
    "def record_chunk(p, stream, file_path, chunk_length=15):\n",
    "    frames = []\n",
    "    print('Generating audio file')\n",
    "    for _ in range(0, int(16000 / 1024 * chunk_length)):\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "    wf = wave.open(file_path, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(2)#p.get_sample_size(pyaudio.paInt16))\n",
    "    wf.setframerate(16000)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model_size = \"medium\"\n",
    "    model = WhisperModel(model_size, device=\"cpu\",compute_type=\"int8\")\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=44100, input=True, frames_per_buffer=1024)\n",
    "\n",
    "    accumulated_transcription= \"\"\n",
    "    print('Intializing')\n",
    "    i = 0\n",
    "    try:\n",
    "        while True:\n",
    "            i +=1\n",
    "            chunk_file=f\"temp/temp_chunk_{i}.wav\"\n",
    "            record_chunk(p, stream, chunk_file)\n",
    "            results, info = model.transcribe(chunk_file)\n",
    "            for transcription in results:\n",
    "                accumulated_transcription += transcription.text + \" \"\n",
    "                print(f\"Transcription: {transcription.text}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping...\")\n",
    "        with open(\"log.txt\", \"w\") as log_file:\n",
    "            log_file.write(f\"{accumulated_transcription}\")\n",
    "    finally:\n",
    "        print(\"Record stopped\")\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intializing\n",
      "Generating audio file\n",
      "Transcription:  Thank you.\n",
      "Generating audio file\n",
      "Stopping...\n",
      "Record stopped\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from pynput import keyboard\n",
    "from scipy.io.wavfile import write\n",
    "import tempfile\n",
    "import os\n",
    "from faster_whisper import WhisperModel\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"./huggingface_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperTranscriber:\n",
    "    def __init__(self, model_size=\"large-v2\", sample_rate=44100):\n",
    "        self.model_size= model_size\n",
    "        self.sample_rate = sample_rate\n",
    "        self.model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "        self.is_recording = False\n",
    "    \n",
    "    def on_press(self, key):\n",
    "        if key == keyboard.Key.space:\n",
    "            if not self.is_recording:\n",
    "                self.is_recording = True\n",
    "                print(\"Recording started\")\n",
    "    \n",
    "    def on_release(self, key):\n",
    "        if key == keyboard.Key.space:\n",
    "            if self.is_recording:\n",
    "                self.is_recording = False\n",
    "                print(\"Recording stopped\")\n",
    "                return False\n",
    "    def record_audio(self):\n",
    "        recording = np.array([], dtype='float64').reshape(0, 2)\n",
    "        frames_per_buffer =int(self.sample_rate * 0.5)\n",
    "\n",
    "        with keyboard.Listener(on_press = self.on_press, on_release = self.on_release) as listener:\n",
    "            while True:\n",
    "                if self.is_recording:\n",
    "                    chunk = sd.rec(frames_per_buffer, samplerate=self.sample_rate, channels=2, dtype='float32')\n",
    "                    sd.wait()\n",
    "                    recording = np.vstack([recording, chunk])\n",
    "                if not self.is_recording and len(recording) > 0:\n",
    "                    break\n",
    "            listener.join()\n",
    "        return recording\n",
    "    \n",
    "    def save_temp_audio(self, recording, temp_path = 'temp/'):\n",
    "        os.makedirs(temp_path, exist_ok=True) \n",
    "        print(\"Estoy guardando el audio\")\n",
    "        temp_file = tempfile.NamedTemporaryFile(dir=temp_path,delete=False, suffix='.wav')\n",
    "        write( temp_file.name, self.sample_rate, recording)\n",
    "        int16_recording = np.int16(recording * 32767)\n",
    "        temp_file_int = tempfile.NamedTemporaryFile(dir=temp_path,delete=False, suffix='.wav')\n",
    "        write( temp_file_int.name, self.sample_rate, int16_recording)\n",
    "        return temp_file.name\n",
    "    \n",
    "    def transcribe_audio(self, file_path):\n",
    "        print(file_path)\n",
    "        segments, info = self.model.transcribe(file_path, beam_size=5)\n",
    "        print(f\"Detected language '{info.language}' with probability {info.language_probability}\")\n",
    "        full_transcription = \"\"\n",
    "        for segment in segments:\n",
    "            full_transcription += segment.text + \" \"\n",
    "        # os.remove(file_path)\n",
    "        return full_transcription\n",
    "        \n",
    "    def run(self):\n",
    "        print(\"Hold the spacebar to start recording...\")\n",
    "        while True:\n",
    "            recording = self.record_audio()\n",
    "            file_path = self.save_temp_audio(recording)\n",
    "            self.transcribe_audio(file_path)\n",
    "            print(\"Press the spacebar to start recording again, or press Ctl+C to exit.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber = WhisperTranscriber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold the spacebar to start recording...\n",
      "Recording started\n",
      "Recording stopped\n",
      "Estoy guardando el audio\n",
      "c:\\Users\\Enrique\\OneDrive\\Documentos\\ai_voice_recognition\\temp\\tmp3llvej6_.wav\n",
      "Detected language 'en' with probability 0.41731569170951843\n",
      "Press the spacebar to start recording again, or press Ctl+C to exit.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtranscriber\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[125], line 58\u001b[0m, in \u001b[0;36mWhisperTranscriber.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHold the spacebar to start recording...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     recording \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_temp_audio(recording)\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranscribe_audio(file_path)\n",
      "Cell \u001b[1;32mIn[125], line 25\u001b[0m, in \u001b[0;36mWhisperTranscriber.record_audio\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m frames_per_buffer \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m keyboard\u001b[38;5;241m.\u001b[39mListener(on_press \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_press, on_release \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_release) \u001b[38;5;28;01mas\u001b[39;00m listener:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_recording:\n\u001b[0;32m     27\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m sd\u001b[38;5;241m.\u001b[39mrec(frames_per_buffer, samplerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transcriber.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: es with probability 1\n",
      "[0.00s - 3.00s]:  Subtítulos realizados por la comunidad de Amara.org\n",
      "Segment start: 0.0, end: 3.0\n",
      "Text:  Subtítulos realizados por la comunidad de Amara.org\n",
      "Tokens: [50364, 8511, 83, 6712, 28348, 22828, 4181, 1515, 635, 35695, 368, 2012, 2419, 13, 4646, 50514]\n",
      "Avg logprob: -0.3144052168902229\n",
      "Compression ratio: 0.8813559322033898\n",
      "No speech prob: 0.8903958201408386\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Ruta al archivo generado\n",
    "audio_path = \"temp/output_normalized.wav\"\n",
    "\n",
    "# Carga el modelo\n",
    "model = WhisperModel(\"large-v2\", device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "# Transcribe el audio\n",
    "segments, info = model.transcribe(audio_path, language=\"es\", beam_size=5, no_speech_threshold=0.9)\n",
    "\n",
    "# Muestra los resultados\n",
    "print(f\"Detected language: {info.language} with probability {info.language_probability}\")\n",
    "for segment in segments:\n",
    "    print(f\"[{segment.start:.2f}s - {segment.end:.2f}s]: {segment.text}\")\n",
    "    print(f\"Segment start: {segment.start}, end: {segment.end}\")\n",
    "    print(f\"Text: {segment.text}\")\n",
    "    print(f\"Tokens: {segment.tokens}\")\n",
    "    print(f\"Avg logprob: {segment.avg_logprob}\")\n",
    "    print(f\"Compression ratio: {segment.compression_ratio}\")\n",
    "    print(f\"No speech prob: {segment.no_speech_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se pudo entender el audio\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Usar Sphinx para reconocer desde un archivo de audio\n",
    "with sr.AudioFile(\"temp/output.wav\") as source:\n",
    "    audio = recognizer.record(source)\n",
    "\n",
    "\n",
    "\n",
    "# Usar Google Speech Recognition directamente\n",
    "try:\n",
    "    text = recognizer.recognize_google(audio, language=\"es-ES\")\n",
    "    print(f\"Texto transcrito: {text}\")\n",
    "except sr.UnknownValueError:\n",
    "    print(\"No se pudo entender el audio\")\n",
    "except sr.RequestError as e:\n",
    "    print(f\"No se pudo conectar al servicio de Google Speech Recognition; {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "incorrect audio shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m mel \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mlog_mel_spectrogram(audio)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Detectar el lenguaje\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Whisper tiene un método de detección de lenguaje interno\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m lang_probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_language\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m language \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(lang_probs, key\u001b[38;5;241m=\u001b[39mlang_probs\u001b[38;5;241m.\u001b[39mget)  \u001b[38;5;66;03m# Detecta el idioma con la mayor probabilidad\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected language: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with probability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang_probs[language]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Enrique\\OneDrive\\Documentos\\ai_voice_recognition\\pyenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Enrique\\OneDrive\\Documentos\\ai_voice_recognition\\pyenv\\Lib\\site-packages\\whisper\\decoding.py:52\u001b[0m, in \u001b[0;36mdetect_language\u001b[1;34m(model, mel, tokenizer)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# skip encoder forward pass if already-encoded audio features were given\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m!=\u001b[39m (model\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_ctx, model\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_state):\n\u001b[1;32m---> 52\u001b[0m     mel \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# forward pass using a single token, startoftranscript\u001b[39;00m\n\u001b[0;32m     55\u001b[0m n_audio \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Enrique\\OneDrive\\Documentos\\ai_voice_recognition\\pyenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Enrique\\OneDrive\\Documentos\\ai_voice_recognition\\pyenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Enrique\\OneDrive\\Documentos\\ai_voice_recognition\\pyenv\\Lib\\site-packages\\whisper\\model.py:197\u001b[0m, in \u001b[0;36mAudioEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    194\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[0;32m    195\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincorrect audio shape\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n",
      "\u001b[1;31mAssertionError\u001b[0m: incorrect audio shape"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "# Cargar el modelo Whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Ruta del archivo de audio\n",
    "audio_path = \"temp/output_normalized.wav\"\n",
    "\n",
    "# Cargar el audio y obtener el espectrograma de Mel\n",
    "audio = whisper.load_audio(audio_path)\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# Detectar el lenguaje\n",
    "# Whisper tiene un método de detección de lenguaje interno\n",
    "lang_probs = model.detect_language(mel)\n",
    "language = max(lang_probs, key=lang_probs.get)  # Detecta el idioma con la mayor probabilidad\n",
    "print(f\"Detected language: {language} with probability: {lang_probs[language]:.2f}\")\n",
    "\n",
    "# Crear las opciones para la decodificación (ya que el idioma está detectado, lo pasamos aquí)\n",
    "options = whisper.DecodingOptions(fp16=False, language=language, beam_size=5)\n",
    "\n",
    "# Decodificar el audio\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# Mostrar el resultado de la transcripción\n",
    "print(f\"Transcription: {result.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2.87G/2.87G [14:08<00:00, 3.64MiB/s]\n",
      "c:\\Users\\Enrique\\OneDrive\\Documentos\\ai_voice_recognition\\pyenv\\Lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "c:\\Users\\Enrique\\OneDrive\\Documentos\\ai_voice_recognition\\pyenv\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"large-v2\")\n",
    "\n",
    "# load the entire audio file\n",
    "audio = whisper.load_audio(audio_path)\n",
    "\n",
    "options = {\n",
    "    \"language\": \"es\", # input language, if omitted is auto detected\n",
    "    \"task\": \"translate\" # or \"transcribe\" if you just want transcription\n",
    "}\n",
    "result = whisper.transcribe(model, audio, **options)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frecuencia\n",
      "mono\n",
      "Transcribing...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model=\"medium\"\n",
    "audio_model = whisper.load_model(model, device=\"cpu\")\n",
    "audio_segment = AudioSegment.from_wav(audio_path)\n",
    "#audio_segment = audio_segment.low_pass_filter(1000)\n",
    "# get sample rate\n",
    "if audio_segment.frame_rate != 16000: # 16 kHz\n",
    "    print('Frecuencia')\n",
    "    audio_segment = audio_segment.set_frame_rate(16000)\n",
    "if audio_segment.sample_width != 2:   # int16\n",
    "    print('canales')\n",
    "    audio_segment = audio_segment.set_sample_width(2)\n",
    "if audio_segment.channels != 1:       # mono\n",
    "    print('mono')\n",
    "    audio_segment = audio_segment.set_channels(1)        \n",
    "arr = np.array(audio_segment.get_array_of_samples())\n",
    "arr = arr.astype(np.float32)/32768.0\n",
    "# to device\n",
    "print(f\"Transcribing...\")\n",
    "result = audio_model.transcribe(arr, language=\"es\", fp16=torch.cuda.is_available())\n",
    "text = result['text'].strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Enrique\\OneDrive\\Documentos\\ai_voice_recognition\\pyenv\\Lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold the spacebar to start recording...\n",
      "Recording started...\n",
      "Recording stopped.\n",
      "Transcribing file: c:\\Users\\Enrique\\OneDrive\\Documentos\\ai_voice_recognition\\temp\\tmptn7pv15i.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Enrique\\OneDrive\\Documentos\\ai_voice_recognition\\pyenv\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Hola, mi nombre es Enrique Delgado Aznar y mi DNI es 778-416-99Q.\n",
      "\n",
      "Press the spacebar to record again, or Ctrl+C to exit.\n",
      "Hold the spacebar to start recording...\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import whisper\n",
    "import tempfile\n",
    "import os\n",
    "from pynput import keyboard\n",
    "\n",
    "\n",
    "class WhisperTranscriber:\n",
    "    def __init__(self, model_size=\"medium\", sample_rate=16000):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.model_size = model_size\n",
    "        self.model = whisper.load_model(self.model_size)\n",
    "        self.is_recording = False\n",
    "        self.recording = []  # Para almacenar los datos grabados\n",
    "\n",
    "    def on_press(self, key):\n",
    "        if key == keyboard.Key.space and not self.is_recording:\n",
    "            self.is_recording = True\n",
    "            self.recording = []  # Resetea la grabación\n",
    "            print(\"Recording started...\")\n",
    "\n",
    "    def on_release(self, key):\n",
    "        if key == keyboard.Key.space and self.is_recording:\n",
    "            self.is_recording = False\n",
    "            print(\"Recording stopped.\")\n",
    "            return False  # Salir del listener\n",
    "\n",
    "    def record_audio(self, duration=15):\n",
    "        print(\"Hold the spacebar to start recording...\")\n",
    "        with keyboard.Listener(on_press=self.on_press, on_release=self.on_release) as listener:\n",
    "            while True:\n",
    "                if self.is_recording:\n",
    "                    # Grabar en chunks pequeños (streaming)\n",
    "                    chunk = sd.rec(int(self.sample_rate * duration), samplerate=self.sample_rate, channels=1, dtype=\"int16\")\n",
    "                    sd.wait()  # Esperar a que el chunk termine\n",
    "                    self.recording.append(chunk)\n",
    "                else:\n",
    "                    if len(self.recording) > 0:\n",
    "                        break\n",
    "            listener.join()\n",
    "\n",
    "        # Combinar todos los chunks grabados\n",
    "        full_recording = np.concatenate(self.recording, axis=0)\n",
    "        return full_recording\n",
    "\n",
    "    def save_temp_audio(self, recording, temp_path=\"temp/\"):\n",
    "        os.makedirs(temp_path, exist_ok=True)\n",
    "        with tempfile.NamedTemporaryFile(dir=temp_path, delete=False, suffix=\".wav\") as temp_file:\n",
    "            # Guardar la grabación directamente como WAV\n",
    "            from scipy.io.wavfile import write\n",
    "            write(temp_file.name, self.sample_rate, recording)\n",
    "            return temp_file.name\n",
    "\n",
    "    def transcribe_audio(self, file_path):\n",
    "        print(f\"Transcribing file: {file_path}\")\n",
    "        result = self.model.transcribe(file_path, language=\"es\")\n",
    "        print(f\"Transcription: {result['text']}\")\n",
    "        os.remove(file_path)  # Limpiar archivo temporal\n",
    "        return result['text']\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            while True:\n",
    "                audio_data = self.record_audio()\n",
    "                temp_audio_path = self.save_temp_audio(audio_data)\n",
    "                transcription = self.transcribe_audio(temp_audio_path)\n",
    "                print(\"\\nPress the spacebar to record again, or Ctrl+C to exit.\")\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Exiting...\")\n",
    "            return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transcriber = WhisperTranscriber()\n",
    "    transcriber.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabando...\n",
      "Grabación completada. Guardando archivo...\n",
      "Grabación guardada como test_recording.wav\n",
      "Reproduciendo la grabación...\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import numpy as np\n",
    "import wave\n",
    "\n",
    "def record_and_playback(sample_rate=44100, duration=5, output_file=\"test_recording.wav\"):\n",
    "    print(\"Grabando...\")\n",
    "    recording = sd.rec(int(sample_rate * duration), samplerate=sample_rate, channels=1, dtype='int16')\n",
    "    sd.wait()  # Esperar a que termine la grabación\n",
    "    print(\"Grabación completada. Guardando archivo...\")\n",
    "\n",
    "    # Guardar archivo WAV\n",
    "    write(output_file, sample_rate, recording)\n",
    "    print(f\"Grabación guardada como {output_file}\")\n",
    "    \n",
    "    # Reproducir el archivo grabado para verificar la calidad\n",
    "    print(\"Reproduciendo la grabación...\")\n",
    "    with wave.open(output_file, 'rb') as wf:\n",
    "        sample_rate = wf.getframerate()\n",
    "        data = wf.readframes(wf.getnframes())\n",
    "        audio = np.frombuffer(data, dtype='int16')\n",
    "        sd.play(audio, samplerate=sample_rate)\n",
    "        sd.wait()  # Esperar a que termine la reproducción\n",
    "\n",
    "record_and_playback()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
